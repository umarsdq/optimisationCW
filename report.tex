\documentclass{article}
\usepackage{amsmath} % For advanced math environments
\usepackage{graphicx} % To include figures
\usepackage{booktabs} % For professional-looking tables
\usepackage{geometry} % To set page margins
\usepackage{amssymb} % For \mathbb and other symbols
\usepackage{float}
\geometry{a4paper, margin=1in}
\setlength{\parskip}{1em} % vertical space between paragraphs
\setlength{\parindent}{0pt} % remove indentation

\title{Computational Linear Algebra: Coursework 1 Report}
\author{CID: 02238728 / ID: us322}
\date{\today}

\begin{document}

\maketitle

\section*{Task 1: Polynomial Fitting}

\subsection*{(a) Formulation of the Weighted Least Squares Problem}

The objective is to fit a polynomial $p(t)$ to a set of noisy measurements $y_i$ at points $t_i$. The noise is applied such that the measurements in the second half of the interval are much noisier.

The weighted least squares method seeks to find the coefficients of the polynomial, $c_0, \dots, c_n$, that minimize the weighted sum of squared residuals with weights $w_i$:
\[
\min_{c_0, \dots, c_n} \sum_{i=1}^{m} w_i (p(t_i) - y_i)^2
\]
where the polynomial $p(t)$ is defined as:
\[
p(t) = \sum_{j=0}^{n} c_j t^j
\]
This minimization problem can be formulated as a linear algebra problem. The Vandermonde matrix $A$, the coefficient vector $c$, and the observation vector $y$ are defined as follows:
\[
A = 
\begin{pmatrix}
1 & t_1 & t_1^2 & \dots & t_1^n \\
1 & t_2 & t_2^2 & \dots & t_2^n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & t_m & t_m^2 & \dots & t_m^n
\end{pmatrix},
\quad
c = 
\begin{pmatrix}
c_0 \\ c_1 \\ \vdots \\ c_n
\end{pmatrix},
\quad
y = 
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_m
\end{pmatrix}
\]
The problem is then equivalent to minimizing the 2-norm 
\[
\| W(Ac - y) \|_2^2
\]
The matrix $W$ must be chosen such that $W^T W$ is the diagonal matrix of weights $w_i$. Since the weights are given by $w_i = 1/\sigma_i^2$, where $\sigma_i$ is the standard deviation of the noise at measurement $i$, the entries are equal to the square root of the weights:
\[
W_{ii} = \sqrt{w_i} = \sqrt{1/\sigma_i^2} = 1/\sigma_i
\]
By setting $A^\prime = WA$ and $y^\prime = Wy$, the problem transforms into a standard least squares problem:
\[
\min_{c} \| A^\prime c - y^\prime \|_2^2
\]
The implementation for this is provided in the function \texttt{solve\_wls()} in the file \texttt{cw1/q1.py}.

\newpage
\subsection*{(b) Weighted Least Squares Fit (n=5)}

The weighted least squares problem, $\min_{c} \|A^\prime c - y^\prime\|_2$, can be solved using QR factorization, yielding $A^\prime = QR$. This transforms the problem into minimizing
\[
\|QRc - y^\prime\|_2
\]
which is equivalent to solving the upper triangular system 
\[
Rc = Q^T y^\prime
\]
The $R$ and $Q^T y^\prime$ matrices can be obtained directly from the QR factorization of an augmented matrix $A_{aug} = [A^\prime | y^\prime]$.  If we compute the factorization $A_{aug} = Q_{aug}R_{aug}$, then the upper triangular matrix $R$ for the original system is the leading $(n+1) \times (n+1)$ block of $R_{aug}$, and the vector $Q^T y^\prime$ is given by the first $(n+1)$ entries of the last column of $R_{aug}$. 
The above upper triangular system is then constructed and solved for the coefficient vector $c_5$ using the \texttt{cla\_utils.solve\_U} function. The resulting polynomial fit is found by passing the coefficients through the \texttt{np.poly1d()} function. This implementation is found in the \texttt{solve\_wls()} function in \texttt{cw1/q1.py}.

To quantify the quality of the polynomial fit, the weighted mean squared error (WMSE) between the fit $p(t)$ and the noisy data $y$ is computed
\[
\text{WMSE} = \frac{1}{m} \sum_{i=1}^{m} \frac{(p(t_i) - y_i)^2}{\sigma_i^2}
\]
for the $i$-th sample points $t_i$. A lower WMSE value indicates a better fit to the data.

\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{plots/plot_Weighted least squares fit for m=20, n=5.png}
\caption{Weighted least squares fit for $m=20$, $n=5$.}
\label{fig:n5_fit}
\end{figure}

The $n=5$ polynomial fit is plotted with the noisy data and true function in Figure \ref{fig:n3_fit}. The polynomial fits the data poorly, particularly in the second half of the interval, where the data is most noisy.

To investigate effect of increasing the number of sample points $m$ on the quality of the fit, we use a for loop to generate data points for varying $m$ values and compute the $n=5$ polynomial fit and corresponding WMSE for each $m$ value. The WMSE values are plotted against the $m$ values in Figure \ref{fig:m_vary}.

\begin{figure}[H]
\centering
\includegraphics[width=0.625\textwidth]{plots/plot_WMSE_vs_m.png}
\caption{Effect of WMSE on varying number of sample points, m}
\label{fig:m_vary}
\end{figure}

As we increase $m$, the WMSE generally decreases up to a point. This is because as $m$ increases, the larger sample better represents the underlying noise distribution, particularly in the noisier second half of the interval, and the polynomial coefficients stabilize. The WMSE appears to converge for $m>1000$ as random fluctuations average out with more data points.

\subsection*{(c) Weighted Least Squares Fit (n=3)}

For $n=3$, the problem can be solved efficiently by reusing the factorization computed for $n=5$. The upper triangular matrix $R_3$ is the leading $4 \times 4$ block of $R_5$ (which itself is a block of $R_{aug,5}$). Similarly, the vector $Q_3^T y'$ corresponds to the first four entries of the vector $Q_5^T y'$. Both of these components can be extracted directly from the $R_{aug,5}$ matrix computed previously. The coefficients $c_3$, and therefore the polynomial fit, are then found by solving the resulting triangular system $R_3 C_3 = Q_3^T y^\prime$ as before.

As expected, the $n=3$ polynomial poorly fits the data points with a significantly higher WMSE value.

\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{plots/plot_Comparison of weighted least squares fits (n=5 and n=3).png}
\caption{Comparison of weighted least squares fits for $n=3,5$.}
\label{fig:n3_fit}
\end{figure}

\subsection*{(d) Weighted Least Squares Fit (n=7)}

To solve for $n=7$, the existing QR factorization for $n=5$ is extended efficiently without recomputing from scratch. The weighted Vandermonde matrix $A^\prime_7$ is partitioned as a block matrix:
\[
A^\prime_7 = [A^\prime_5 \mid A^\prime_{2}]
\]
where $A^\prime_5 \in \mathbb{R}^{m \times 6}$ contains the already factorized columns for powers $t^0, \dots, t^5$, and $A^\prime_{2} \in \mathbb{R}^{m \times 2}$ contains the new columns for $t^6$ and $t^7$.

As we already have $A^\prime_5 = Q_5 R_5$ from part (b), we seek a block QR factorization of the form:
\[
A^\prime_7 = [Q_5 \mid Q_{2}] \begin{bmatrix} R_5 & R_{12} \\ 0 & R_{22} \end{bmatrix} = Q_7 R_7
\]
where $Q_{2} \in \mathbb{R}^{m \times 2}$ contains orthonormal columns orthogonal to $Q_5$, and $R_7 \in \mathbb{R}^{8 \times 8}$ is the extended upper triangular matrix.

The upper-right block is computed by projecting the new columns onto the existing basis:
\[
R_{12} = Q_5^T A^\prime_{2}
\]
The new columns are then orthogonalized against $Q_5$. The remaining part after removing the projection onto $Q_5$ is given by:
\[
A^\prime_{2} - Q_5 R_{12}
\]
Applying Gram-Schmidt to this remaining part yields $Q_{2}$ and $R_{22}$. The right-hand side vector $Q_7^T y^\prime$ is constructed block-wise without forming the full $Q_7$ matrix:
\[
Q_7^T y^\prime = \begin{bmatrix} Q_5^T y^\prime \\ Q_{2}^T y^\prime \end{bmatrix}
\]
where $Q_5^T y^\prime$ is already available from the augmented factorization in part (b), and only $Q_{2}^T y^\prime$ needs to be computed through Modified Gram-Schmidt. The polynomial fit is then determined by solving the upper triangular system $R_7 c_7 = Q_7^T y^\prime$. To confirm the validity of this approach, the coefficients are also verified with the direct approach as used in part (b).

\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{plots/plot_Comparison of fits for larger dataset (n=3, n=5, and n=7).png}
\caption{Comparison of weighted least squares fits ($n=3,5,7$) for larger dataset ($m=100$)}
\label{fig:n7_comparison}
\end{figure}

The polynomial fits for $n=3,5,7$ are visualized together for a larger dataset ($m=100$) in Figure \ref{fig:n7_comparison} for comparison. The WMSE values suggest that the fit improves significantly with $n=7$. However, while the $n=7$ polynomial fits the data for $t<0.5$ very well, it fails to reflect the noisy distribution in the second half of the interval. The $n=5$ fit is also the only polynomial to fit the noisy data in the $t>0.8$ tail region well.

\subsection*{(e) Comparison with Unweighted Least Squares Solution}

The unweighted least squares problem is a special case of the weighted version where $w_i=1$. In this case, the weighting matrix $W$ becomes the identity matrix, and the problem simplifies to the standard least squares formulation:
\[
\min_{c} \| Ac - y \|_2^2
\]
The \texttt{solve\_wls()} function implements this by defaulting to \texttt{sigma=None}, such that when no sigma is provided, sigma is initialized to an array of ones, equivalent to setting $w_i=1$. 

Figure \ref{fig:noise_grid} shows the weighted and unweighted fits for varying polynomial degrees ($n=3,5,7$) and noise scales ($1.0,3.0,5.0$). To quantitatively compare the quality of the fits, both methods are evaluated using the weighted mean squared error (WMSE) with the same weights $w_i = 1/\sigma_i^2$.

The weighted LS achieves consistently lower WMSE than unweighted LS across all polynomial degrees and noise scales. Visually, the weighted LS fit follows the less noisy data in the first half ($t<0.5$) more closely, while the unweighted LS achieves a more balanced fit across the entire interval. This is because the unweighted approach assigns equal importance to all data points, whereas the weighted approach assigns higher importance to data points with lower variance (i.e., more reliable measurements). As noise scale increases, both methods struggle, particularly in the second half ($t>0.5$) of the interval.

The choice of weights $w_i = 1/\sigma_i^2$ is statistically justified when measurement reliability varies significantly, and is therefore appropriate in our case where the data is much noisier in the second half of the interval.
\begin{figure}[H]
\centering
\begin{tabular}{ccc}
\includegraphics[width=0.317\textwidth]{plots/plot_n=3, noise scale = 1.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=3, noise scale = 3.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=3, noise scale = 5.0.png} \\
\includegraphics[width=0.317\textwidth]{plots/plot_n=5, noise scale = 1.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=5, noise scale = 3.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=5, noise scale = 5.0.png} \\
\includegraphics[width=0.317\textwidth]{plots/plot_n=7, noise scale = 1.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=7, noise scale = 3.0.png} &
\includegraphics[width=0.317\textwidth]{plots/plot_n=7, noise scale = 5.0.png} \\
\end{tabular}
\caption{Comparison of weighted and unweighted least squares fits for different polynomial degrees ($n=3, 5, 7$) and noise scales ($1.0, 3.0, 5.0$).}
\label{fig:noise_grid}
\end{figure}

\newpage
\section*{Task 2: Orthogonality of QR Factorizations}

\subsection*{(a) Implementing QR Factorization Variants}

Five variants of QR factorization were implemented, with the Classical and Modified Gram-Schmidt, as well as the Householder algorithm, reused from the \texttt{cla\_utils} library. The Double and Triple Gram-Schmidt methods are enhancements designed to counteract the loss of orthogonality that occurs in Classical Gram-Schmidt (CGS) due to round-off errors. The code in \texttt{cw1/q2.py} implements the DGS and TGS algorithms using an iterated Gram-Schmidt function, \texttt{iterated\_GS()}, that accepts a specified number of orthogonalization steps and follows the algorithm provided in the coursework instructions.

\begin{itemize}
    \item{\textbf{Double Gram-Schmidt (DGS)} performs the CGS process twice for each vector. The second pass (re-orthogonalization) removes any remaining components in the direction of the basis vectors not eliminated in the first pass due to numerical inaccuracies.}
    \item{\textbf{Triple Gram-Schmidt (TGS)} extends this idea by performing the re-orthogonalization step a second time, for a total of three passes.}
\end{itemize}

\subsection*{(b) Orthogonality Error Results}
For each of the five Gram-Schmidt variants, $A=QR$ was factorized, and $orth = ||Q^TQ-I||_2$ was computed.

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{$\lVert Q^T Q - I\rVert_2$} \\
\midrule
Classical GS    & $9.29\times 10^{1}$ \\
Modified GS     & $4.93\times 10^{-10}$ \\
Double GS       & $4.37\times 10^{-15}$ \\
Triple GS     & $4.29\times 10^{-15}$ \\
Householder       & $1.41\times 10^{-14}$ \\
\bottomrule
\end{tabular}
\label{table_1}
\caption{Orthogonality Error for QR Variants}
\end{table}

\subsection*{(c) Discussion of Orthogonality Error}

The Classical Gram-Schmidt (CGS) algorithm exhibits a very large orthogonality error ($\sim 10^{2}$), indicating significant loss of orthogonality due to the accumulation of rounding errors during the projection steps. In contrast, the Modified Gram-Schmidt (MGS) algorithm achieves a much smaller error ($\sim 10^{-10}$), as it performs projections in a more numerically stable order, thereby reducing the propagation of floating-point inaccuracies. The Householder method achieves an error $\sim 10^{-14}$, close to machine precision, as it constructs orthogonal reflectors that inherently maintain numerical stability. 

The newly implemented Double Gram-Schmidt (DGS) method substantially improves upon the CGS algorithm by performing a re-orthogonalization pass, which effectively corrects for the loss of orthogonality from the first pass, resulting in accuracy comparable to that of the Householder algorithm. The Triple Gram-Schmidt (TGS) method yields a similar level of orthogonality to DGS ($\sim 10^{-15}$), showing that once machine precision has been reached, additional re-orthogonalization offers negligible improvement.

\newpage
\section*{Mastery Question: Lower Triangular Factorization}
\subsubsection*{CID: 02238728 / ID: us322}

\subsection*{(a) Algorithm Derivation and Implementation}

The objective is to construct an orthogonal matrix $Q \in \mathbb{R}^{m \times m}$ such that for a given matrix $A \in \mathbb{R}^{m \times n}$ (with $m \geq n$), the product $Q^T A$ has a lower trapezoidal structure:
\[
Q^T A =
\begin{bmatrix}
0 \\
L
\end{bmatrix}
\]
where the zero block is $(m-n) \times n$ and $L \in \mathbb{R}^{n \times n}$ is lower triangular.

The algorithm processes columns from right to left ($k = n-1, \dots, 0$), applying Householder reflectors $Q_k = I - 2vv^T$ to build the lower triangular structure. At iteration $k$, we extract $x = A[:m-n+k+1, k]$ and construct the normalized Householder vector $v = x + \text{sign}(x[-1])\|x\|e$, where $e$ is the last standard basis vector and $x[-1]$ denotes the last element of $x$. 

The matrices  $A$ and $Q$ are then updated via:
\begin{align*}
A &\leftarrow A - 2v(v^T A) \\
Q &\leftarrow Q - 2(Qv)v^T
\end{align*}
starting from $Q = I$. After all iterations, $Q = Q_{n-1} \cdots Q_1 Q_0$, and $A$ is transformed into $Q^T A$.

This algorithm is implemented in the function \texttt{householder\_ql()} in the \texttt{cw1/mastery.py} script.

\subsection*{(b) Relation to QR Factorization of $A^T$}

We can partition the orthogonal matrix $Q$ into
$Q = \begin{bmatrix} Q_1 & Q_2 \end{bmatrix}$, where $Q_1 \in \mathbb{R}^{m \times (m-n)}$ and $Q_2 \in \mathbb{R}^{m \times n}$.

Left-multiplying the original equation by $Q$ gives:
\[
A = Q \begin{bmatrix} 0 \\ L \end{bmatrix} = \begin{bmatrix} Q_1 & Q_2 \end{bmatrix} \begin{bmatrix} 0 \\ L \end{bmatrix} = Q_2 L
\]
This is the (thin) QL factorization of $A$. Since $Q_2$ is an $m \times n$ matrix with orthonormal columns and $L$ is an $n \times n$ invertible matrix (assuming $A$ has full rank), this equation shows that every column of $A$ is a linear combination of the columns of $Q_2$.

Therefore, the columns of $Q_2$ form an orthonormal basis for the column space of $A$, $\mathcal{C}(A)$.

Now, consider the standard QR factorization of $A^T \in \mathbb{R}^{n \times m}$:
\[
A^T = \tilde{Q} \tilde{R}
\]
where $\tilde{Q} \in \mathbb{R}^{n \times n}$ is orthogonal and $\tilde{R} \in \mathbb{R}^{n \times m}$ is upper trapezoidal.

By the definition of the QR factorization, the columns of $\tilde{Q}$ form an orthonormal basis for the column space of the matrix being factored, which in this case is $A^T$.

Therefore, the columns of $\tilde{Q}$ form an orthonormal basis for the column space of $A^T$, $\mathcal{C}(A^T)$. By definition, the column space of $A^T$ is the row space of $A$, $\mathcal{R}(A)$.

Hence the two Q-matrices, $Q$ and $\tilde{Q}$, are related through the fundamental subspaces of $A$. In summary,
\begin{itemize}
    \item The last $n$ columns of $Q$ (i.e., $Q_2$) provide an orthonormal basis for the column space of $A$.
    \item The $n$ columns of $\tilde{Q}$ provide an orthonormal basis for the row space of $A$.
\end{itemize}

\end{document}